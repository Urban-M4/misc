#!/bin/bash
#SBATCH --job-name=wrf_experiment     # Job name
#SBATCH --partition=genoa              # Request thin partition. Up to one hour goes to fast queue
#SBATCH --time=00:01:00                # Maximum runtime (D-HH:MM:SS)
#SBATCH --nodes=1                     # Number of nodes (one thin node has 128 cpu cores)
#SBATCH --ntasks-per-node=24                   # Number of tasks per node / number of patches in the domain - parallelized with MPI / DMPAR / multiprocessing
#SBATCH --cpus-per-task=8             # Number of CPU cores per task / number of tiles within each patch - parallelized with OpenMP / SMPAR / multithreading

# Load dependencies
module purge
module load 2023
module load intel/2023a

WRF_LIBS="/home/pkalverla1/Urban-M4/wrf_libs_intel/"
export PATH=${WRF_LIBS}bin:$PATH
export LD_LIBRARY_PATH=${WRF_LIBS}lib:$LD_LIBRARY_PATH
export NETCDF=$(nf-config --prefix)

wrf_executable="/home/pkalverla1/Urban-M4/wrf_install_intel/WRFV4.6.0/install/run/wrf"

export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
# export OMP_PLACES=cores
# export OMP_PROC_BIND=spread


# https://nrel.github.io/HPC/blog/2021-06-18-srun/#5-threaded-openmp-runs
# https://www.intel.com/content/www/us/en/docs/dpcpp-cpp-compiler/developer-guide-reference/2023-0/thread-affinity-interface.html
export KMP_AFFINITY=compact

# https://www.intel.com/content/www/us/en/docs/mpi-library/developer-guide-linux/2021-13/running-an-mpi-openmp-program.html
# export I_MPI_PIN_DOMAIN=omp

mpirun -np ${SLURM_NTASKS} $wrf_executable



# Fix as per https://community.intel.com/t5/Intel-MPI-Library/Unable-to-run-with-Intel-MPI-on-any-fabric-setting-except-TCP/m-p/1408609
# export I_MPI_OFI_PROVIDER=mlx
# srun $wrf_executable
